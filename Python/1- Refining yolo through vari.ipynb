{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe99ba3-79f1-4328-942f-0fdd890f9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geopandas shapely fiona pyogrio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99c3bb-0edd-460d-87ef-242e1eae4e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e582f-c4a0-443e-93de-9f6553104021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef9c46-96e0-4542-81d6-87b9191cbd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63aba8-edf7-4bbd-a787-6f5d98c91fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b311399-124c-4264-b24a-1936d6b4929f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b58ace-54d7-4d20-ad73-a4e2157f0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import math\n",
    "# import glob\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# from shapely.geometry import box\n",
    "# import fiona\n",
    "# import warnings\n",
    "\n",
    "# # Suppress warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # ================= CONFIGURATION =================\n",
    "# # 1. Folder containing your YOLO shapefiles\n",
    "# #INPUT_YOLO_FOLDER = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\outputRESULTS\"\n",
    "# INPUT_YOLO_FOLDER = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\outputRESULTS\"\n",
    "\n",
    "# # 2. Path to the VARI layer\n",
    "# #VARI_GDB_PATH = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot24\\vari2024.gdb\"\n",
    "# VARI_GDB_PATH = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\plot2022\\VARI2022.gdb\"\n",
    "# #VARI_LAYER_NAME = \"VariIndex2024v3\"\n",
    "# VARI_LAYER_NAME = \"VARI2022\"\n",
    "\n",
    "# # 3. Path to the Plot Boundaries\n",
    "# #INPUT_PLOT_GDB = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot24\\CorrectedPlotDimensions.gdb\"\n",
    "# INPUT_PLOT_GDB = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\plot2022\\CorrectedPlotDimensions.gdb\"\n",
    "\n",
    "# # 4. Output Folder\n",
    "# OUTPUT_FOLDER = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\outputRESULTS\\correctedDetections\\2022\"\n",
    "\n",
    "# # 5. Area settings\n",
    "# SUBDIVIDE_THRESHOLD = 0.028 # Subdivide anything larger than this\n",
    "# MIN_AREA_THRESHOLD = 0.0018  # Eliminate anything smaller than this\n",
    "# ELIMINATION_PASSES = 3       # How many times to try eliminating small polygons      \n",
    "# # =================================================\n",
    "\n",
    "# def subdivide_polygon(geometry, threshold_area):\n",
    "#     \"\"\"\n",
    "#     Splits a large polygon into smaller chunks using a grid (fishnet) approach.\n",
    "#     \"\"\"\n",
    "#     if geometry.area <= threshold_area:\n",
    "#         return [geometry]\n",
    "    \n",
    "#     cell_size = math.sqrt(threshold_area)\n",
    "#     minx, miny, maxx, maxy = geometry.bounds\n",
    "#     chunks = []\n",
    "    \n",
    "#     x = minx\n",
    "#     while x < maxx:\n",
    "#         y = miny\n",
    "#         while y < maxy:\n",
    "#             grid_cell = box(x, y, x + cell_size, y + cell_size)\n",
    "#             chunk = geometry.intersection(grid_cell)\n",
    "            \n",
    "#             if not chunk.is_empty:\n",
    "#                 if chunk.geom_type == 'MultiPolygon':\n",
    "#                     chunks.extend(list(chunk.geoms))\n",
    "#                 elif chunk.geom_type in ['Polygon', 'GeometryCollection']:\n",
    "#                     chunks.append(chunk)\n",
    "#             y += cell_size\n",
    "#         x += cell_size\n",
    "        \n",
    "#     return chunks\n",
    "\n",
    "# def eliminate_pass(gdf, min_area, pass_num):\n",
    "#     \"\"\"\n",
    "#     Single pass of elimination: Explode -> Identify Small -> Merge to Nearest Large.\n",
    "#     \"\"\"\n",
    "#     # Explode\n",
    "#     gdf_exploded = gdf.explode(index_parts=False).reset_index(drop=True)\n",
    "    \n",
    "#     gdf_exploded['temp_area'] = gdf_exploded.geometry.area\n",
    "#     small_polys = gdf_exploded[gdf_exploded['temp_area'] < min_area].copy()\n",
    "#     large_polys = gdf_exploded[gdf_exploded['temp_area'] >= min_area].copy()\n",
    "    \n",
    "#     if small_polys.empty:\n",
    "#         return gdf_exploded.drop(columns=['temp_area'])\n",
    "    \n",
    "#     if large_polys.empty:\n",
    "#         return gdf_exploded.drop(columns=['temp_area'])\n",
    "\n",
    "#     print(f\"    - Pass {pass_num}: Eliminating {len(small_polys)} tiny polygons...\")\n",
    "\n",
    "#     large_polys['merge_id'] = range(len(large_polys))\n",
    "    \n",
    "#     joined = gpd.sjoin_nearest(small_polys, large_polys[['geometry', 'merge_id']], how='left', distance_col='dist')\n",
    "#     small_polys_mapped = joined[['geometry', 'merge_id']].dropna()\n",
    "    \n",
    "#     combined = pd.concat([large_polys[['geometry', 'merge_id']], small_polys_mapped])\n",
    "#     dissolved = combined.dissolve(by='merge_id')\n",
    "    \n",
    "#     return dissolved.reset_index(drop=True)\n",
    "\n",
    "# def main():\n",
    "#     print(\"=== Starting Plant Refinement (Refine + Join Attributes) ===\")\n",
    "    \n",
    "#     if not os.path.exists(OUTPUT_FOLDER):\n",
    "#         os.makedirs(OUTPUT_FOLDER)\n",
    "        \n",
    "#     # ---------------------------------------------------------\n",
    "#     # 1. Scan Plot GDB\n",
    "#     # ---------------------------------------------------------\n",
    "#     print(f\"Scanning Plot GDB: {INPUT_PLOT_GDB}...\")\n",
    "#     plot_lookup = {}\n",
    "    \n",
    "#     try:\n",
    "#         layers = fiona.listlayers(INPUT_PLOT_GDB)\n",
    "#         for layer_name in layers:\n",
    "#             if len(layer_name) >= 4:\n",
    "#                 suffix = layer_name[-4:]\n",
    "#                 plot_lookup[suffix] = layer_name\n",
    "#         print(f\"DEBUG: Found {len(plot_lookup)} valid plot layers.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading Plot GDB: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 2. Load VARI Layer\n",
    "#     # ---------------------------------------------------------\n",
    "#     print(\"Loading VARI layer into memory...\")\n",
    "#     try:\n",
    "#         gdf_vari_master = gpd.read_file(VARI_GDB_PATH, layer=VARI_LAYER_NAME, engine='pyogrio')\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading VARI layer: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # ---------------------------------------------------------\n",
    "#     # 3. Process YOLO Files\n",
    "#     # ---------------------------------------------------------\n",
    "#     yolo_files = glob.glob(os.path.join(INPUT_YOLO_FOLDER, \"*.shp\"))\n",
    "#     print(f\"Found {len(yolo_files)} YOLO files. Starting Loop...\\n\")\n",
    "\n",
    "#     for fp in yolo_files:\n",
    "#         filename = os.path.basename(fp)\n",
    "#         yolo_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "#         try:\n",
    "#             if len(yolo_name) < 4:\n",
    "#                 print(f\"Skipping {yolo_name}: Filename too short.\")\n",
    "#                 continue\n",
    "            \n",
    "#             suffix = yolo_name[-4:]\n",
    "            \n",
    "#             if suffix not in plot_lookup:\n",
    "#                 continue\n",
    "            \n",
    "#             plot_layer_name = plot_lookup[suffix]\n",
    "#             print(f\"Processing: {yolo_name} matches Plot {plot_layer_name}\")\n",
    "            \n",
    "#             # Load Data\n",
    "#             gdf_yolo = gpd.read_file(fp)\n",
    "#             # Read Plot with Attributes\n",
    "#             gdf_plot = gpd.read_file(INPUT_PLOT_GDB, layer=plot_layer_name, engine='pyogrio')\n",
    "            \n",
    "#             if gdf_plot.empty:\n",
    "#                 print(f\"  [!] Plot layer empty. Copying original.\")\n",
    "#                 gdf_yolo.to_file(os.path.join(OUTPUT_FOLDER, filename))\n",
    "#                 continue\n",
    "\n",
    "#             # Align CRS\n",
    "#             target_crs = gdf_plot.crs\n",
    "#             if gdf_yolo.crs != target_crs:\n",
    "#                 gdf_yolo = gdf_yolo.to_crs(target_crs)\n",
    "            \n",
    "#             # Clip VARI\n",
    "#             if gdf_vari_master.crs != target_crs:\n",
    "#                 gdf_plot_vari_crs = gdf_plot.to_crs(gdf_vari_master.crs)\n",
    "#                 gdf_vari_clipped = gpd.clip(gdf_vari_master, gdf_plot_vari_crs)\n",
    "#                 if not gdf_vari_clipped.empty:\n",
    "#                     gdf_vari_clipped = gdf_vari_clipped.to_crs(target_crs)\n",
    "#             else:\n",
    "#                 gdf_vari_clipped = gpd.clip(gdf_vari_master, gdf_plot)\n",
    "\n",
    "#             if gdf_vari_clipped.empty:\n",
    "#                 print(\"  No VARI data in this plot. Saving original.\")\n",
    "#                 gdf_yolo.to_file(os.path.join(OUTPUT_FOLDER, filename))\n",
    "#                 continue\n",
    "\n",
    "#             # Erase\n",
    "#             gdf_erased = gpd.overlay(gdf_vari_clipped, gdf_yolo, how='difference')\n",
    "\n",
    "#             if gdf_erased.empty:\n",
    "#                 print(\"  No missing plants found.\")\n",
    "#                 # We still want to join plot attributes even if no VARI was added!\n",
    "#                 gdf_final_combined = gdf_yolo\n",
    "#             else:\n",
    "#                 # Subdivide\n",
    "#                 gdf_erased['area_sqm'] = gdf_erased.geometry.area\n",
    "#                 small_polys = gdf_erased[gdf_erased['area_sqm'] <= SUBDIVIDE_THRESHOLD].copy()\n",
    "#                 large_polys = gdf_erased[gdf_erased['area_sqm'] > SUBDIVIDE_THRESHOLD].copy()\n",
    "                \n",
    "#                 new_chunks = []\n",
    "#                 if not large_polys.empty:\n",
    "#                     print(f\"  Subdividing {len(large_polys)} large polygons...\")\n",
    "#                     for geom in large_polys.geometry:\n",
    "#                         chunks = subdivide_polygon(geom, SUBDIVIDE_THRESHOLD)\n",
    "#                         new_chunks.extend(chunks)\n",
    "                \n",
    "#                 if new_chunks:\n",
    "#                     gdf_subdivided = gpd.GeoDataFrame(geometry=new_chunks, crs=target_crs)\n",
    "#                     gdf_final_vari = pd.concat([small_polys, gdf_subdivided], ignore_index=True)\n",
    "#                 else:\n",
    "#                     gdf_final_vari = small_polys\n",
    "\n",
    "#                 # Combine\n",
    "#                 gdf_final_combined = pd.concat([gdf_yolo, gdf_final_vari], ignore_index=True)\n",
    "\n",
    "#             # --- ITERATIVE ELIMINATION ---\n",
    "#             current_gdf = gdf_final_combined\n",
    "#             for i in range(ELIMINATION_PASSES):\n",
    "#                 exploded_check = current_gdf.explode(index_parts=False)\n",
    "#                 num_small = len(exploded_check[exploded_check.geometry.area < MIN_AREA_THRESHOLD])\n",
    "#                 if num_small == 0:\n",
    "#                     break\n",
    "#                 current_gdf = eliminate_pass(current_gdf, MIN_AREA_THRESHOLD, i+1)\n",
    "\n",
    "#             # --- NEW: JOIN PLOT ATTRIBUTES ---\n",
    "#             print(\"    - Joining Plot Attributes to Plants...\")\n",
    "#             # Perform Spatial Join: Plants (left) + Plot (right)\n",
    "#             # This attaches columns from gdf_plot to current_gdf\n",
    "#             gdf_attributed = gpd.sjoin(current_gdf, gdf_plot, how='left', predicate='intersects')\n",
    "            \n",
    "#             # Clean up the index column created by sjoin\n",
    "#             if 'index_right' in gdf_attributed.columns:\n",
    "#                 gdf_attributed = gdf_attributed.drop(columns=['index_right'])\n",
    "\n",
    "#             # Save\n",
    "#             out_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "#             gdf_attributed.to_file(out_path)\n",
    "#             print(f\"  Success: Saved to {out_path}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"FAILED on {yolo_name}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     print(\"\\n=== Processing Complete ===\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a8eb8f-03e5-43bc-bfa1-dc73a4182e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Plant Refinement (YOLO Attrs + Fill 0) ===\n",
      "Scanning Plot GDB: C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot2024Canada\\New File Geodatabase.gdb...\n",
      "DEBUG: Found 1 valid plot layers.\n",
      "Loading VARI layer into memory...\n",
      "Found 26 YOLO files. Starting Loop...\n",
      "\n",
      "Processing: Canada24116 matches Plot P24116\n",
      "    - Pass 1: Eliminating 694802 tiny polygons...\n",
      "    - Pass 2: Eliminating 686055 tiny polygons...\n",
      "    - Pass 3: Eliminating 686643 tiny polygons...\n",
      "    - Filling missing 'cls' values with 0...\n",
      "    - Joining Plot Attributes...\n",
      "  Success: Saved to C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\outputRESULTS\\correctedDetections\\Canada24\\Canada24116.shp\n",
      "\n",
      "=== Processing Complete ===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import box\n",
    "import fiona\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. Folder containing your YOLO shapefiles\n",
    "#INPUT_YOLO_FOLDER = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\outputRESULTS\"\n",
    "INPUT_YOLO_FOLDER = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\outputRESULTS\"\n",
    "\n",
    "# 2. Path to the VARI layer\n",
    "#VARI_GDB_PATH = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot24\\vari2024.gdb\"\n",
    "#VARI_GDB_PATH = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\plot2022\\VARI2022.gdb\"\n",
    "#VARI_GDB_PATH = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\plot21mrc\\vari21.gdb\"\n",
    "#VARI_GDB_PATH = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\plot2022\\vari_mrc22.gdb\"\n",
    "#VARI_GDB_PATH = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot2025\\Vari2025.gdb\"\n",
    "#VARI_GDB_PATH = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot2025Canada\\Varicanada25.gdb\"\n",
    "VARI_GDB_PATH = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot2024Canada\\Vari24canada.gdb\"\n",
    "\n",
    "#VARI_LAYER_NAME = \"VariIndex2024v3\"\n",
    "#VARI_LAYER_NAME = \"varisevrec25\"\n",
    "#VARI_LAYER_NAME = \"Vari25canada\"\n",
    "VARI_LAYER_NAME = \"VariCanada24\"\n",
    "\n",
    "# 3. Path to the Plot Boundaries\n",
    "#INPUT_PLOT_GDB = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot24\\CorrectedPlotDimensions.gdb\"\n",
    "#INPUT_PLOT_GDB = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\plot2022\\CorrectedPlotDimensions.gdb\"\n",
    "#INPUT_PLOT_GDB = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\plot21mrc\\New File Geodatabase.gdb\"\n",
    "#INPUT_PLOT_GDB = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\plot2022\\MRC2022.gdb\"\n",
    "#INPUT_PLOT_GDB = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot2025\\25plots_geometry.gdb\"\n",
    "#INPUT_PLOT_GDB = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot2025Canada\\New File Geodatabase.gdb\"\n",
    "INPUT_PLOT_GDB = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Plot2024Canada\\New File Geodatabase.gdb\"\n",
    "\n",
    "\n",
    "\n",
    "# 4. Output Folder\n",
    "#OUTPUT_FOLDER = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\outputRESULTS\\correctedDetections\\2022MRC\"\n",
    "#OUTPUT_FOLDER = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\outputRESULTS\\correctedDetections\\2025\\again\"\n",
    "#OUTPUT_FOLDER = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\outputRESULTS\\correctedDetections\\Canada25\"\n",
    "OUTPUT_FOLDER = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\outputRESULTS\\correctedDetections\\Canada24\"\n",
    "\n",
    "\n",
    "# 5. Area settings\n",
    "SUBDIVIDE_THRESHOLD = 0.028 # Subdivide anything larger than this\n",
    "MIN_AREA_THRESHOLD = 0.0016  # Eliminate anything smaller than this\n",
    "ELIMINATION_PASSES = 3       # How many times to try eliminating small polygons      \n",
    "# =================================================\n",
    "\n",
    "def subdivide_polygon(geometry, threshold_area):\n",
    "    \"\"\"\n",
    "    Splits a large polygon into smaller chunks using a grid (fishnet) approach.\n",
    "    \"\"\"\n",
    "    if geometry.area <= threshold_area:\n",
    "        return [geometry]\n",
    "    \n",
    "    cell_size = math.sqrt(threshold_area)\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "    chunks = []\n",
    "    \n",
    "    x = minx\n",
    "    while x < maxx:\n",
    "        y = miny\n",
    "        while y < maxy:\n",
    "            grid_cell = box(x, y, x + cell_size, y + cell_size)\n",
    "            chunk = geometry.intersection(grid_cell)\n",
    "            \n",
    "            if not chunk.is_empty:\n",
    "                if chunk.geom_type == 'MultiPolygon':\n",
    "                    chunks.extend(list(chunk.geoms))\n",
    "                elif chunk.geom_type in ['Polygon', 'GeometryCollection']:\n",
    "                    chunks.append(chunk)\n",
    "            y += cell_size\n",
    "        x += cell_size\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "def eliminate_pass(gdf, min_area, pass_num):\n",
    "    \"\"\"\n",
    "    Single pass of elimination: \n",
    "    1. Explodes to single parts.\n",
    "    2. Merges small polygons into nearest large neighbor.\n",
    "    3. Preserves attributes of the 'Parent' (Large) polygon.\n",
    "    \"\"\"\n",
    "    # Explode to ensure we analyze individual pieces\n",
    "    gdf_exploded = gdf.explode(index_parts=False).reset_index(drop=True)\n",
    "    \n",
    "    gdf_exploded['temp_area'] = gdf_exploded.geometry.area\n",
    "    \n",
    "    # Identify Small (to be eaten) vs Large (eaters)\n",
    "    small_polys = gdf_exploded[gdf_exploded['temp_area'] < min_area].copy()\n",
    "    large_polys = gdf_exploded[gdf_exploded['temp_area'] >= min_area].copy()\n",
    "    \n",
    "    if small_polys.empty:\n",
    "        return gdf_exploded.drop(columns=['temp_area'])\n",
    "    \n",
    "    if large_polys.empty:\n",
    "        return gdf_exploded.drop(columns=['temp_area'])\n",
    "\n",
    "    print(f\"    - Pass {pass_num}: Eliminating {len(small_polys)} tiny polygons...\")\n",
    "\n",
    "    # Assign a unique merge_id to the Large polygons (Parents)\n",
    "    large_polys['merge_id'] = range(len(large_polys))\n",
    "    \n",
    "    # Find nearest Large neighbor for every Small polygon\n",
    "    # We only care about the geometry and merge_id for the join\n",
    "    joined = gpd.sjoin_nearest(small_polys, large_polys[['geometry', 'merge_id']], how='left', distance_col='dist')\n",
    "    \n",
    "    # The small polygons now know which 'merge_id' they belong to\n",
    "    small_polys_mapped = joined[['geometry', 'merge_id']].dropna()\n",
    "    \n",
    "    # Combine the lists (Large + Mapped Small)\n",
    "    combined = pd.concat([large_polys[['geometry', 'merge_id']], small_polys_mapped])\n",
    "    \n",
    "    # Perform the geometric dissolve\n",
    "    dissolved_geom = combined.dissolve(by='merge_id')\n",
    "    \n",
    "    # --- ATTRIBUTE RESTORATION ---\n",
    "    # The dissolve destroyed attributes. We must join them back from the original Large parents.\n",
    "    # merge_id in 'large_polys' corresponds to the index of 'dissolved_geom'\n",
    "    \n",
    "    # Get original attributes from large_polys (dropping temp columns)\n",
    "    cols_to_keep = [c for c in large_polys.columns if c not in ['geometry', 'temp_area', 'merge_id']]\n",
    "    attributes = large_polys.set_index('merge_id')[cols_to_keep]\n",
    "    \n",
    "    # Join attributes back to the new dissolved geometries\n",
    "    final_gdf = dissolved_geom.join(attributes)\n",
    "    \n",
    "    return final_gdf.reset_index(drop=True)\n",
    "\n",
    "def main():\n",
    "    print(\"=== Starting Plant Refinement (YOLO Attrs + Fill 0) ===\")\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.makedirs(OUTPUT_FOLDER)\n",
    "        \n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Scan Plot GDB\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"Scanning Plot GDB: {INPUT_PLOT_GDB}...\")\n",
    "    plot_lookup = {}\n",
    "    \n",
    "    try:\n",
    "        layers = fiona.listlayers(INPUT_PLOT_GDB)\n",
    "        for layer_name in layers:\n",
    "            if len(layer_name) >= 4:\n",
    "                suffix = layer_name[-4:]\n",
    "                plot_lookup[suffix] = layer_name\n",
    "        print(f\"DEBUG: Found {len(plot_lookup)} valid plot layers.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Plot GDB: {e}\")\n",
    "        return\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Load VARI Layer\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Loading VARI layer into memory...\")\n",
    "    try:\n",
    "        gdf_vari_master = gpd.read_file(VARI_GDB_PATH, layer=VARI_LAYER_NAME, engine='pyogrio')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading VARI layer: {e}\")\n",
    "        return\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. Process YOLO Files\n",
    "    # ---------------------------------------------------------\n",
    "    yolo_files = glob.glob(os.path.join(INPUT_YOLO_FOLDER, \"*.shp\"))\n",
    "    print(f\"Found {len(yolo_files)} YOLO files. Starting Loop...\\n\")\n",
    "\n",
    "    for fp in yolo_files:\n",
    "        filename = os.path.basename(fp)\n",
    "        yolo_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        try:\n",
    "            if len(yolo_name) < 4:\n",
    "                print(f\"Skipping {yolo_name}: Filename too short.\")\n",
    "                continue\n",
    "            \n",
    "            suffix = yolo_name[-4:]\n",
    "            \n",
    "            if suffix not in plot_lookup:\n",
    "                continue\n",
    "            \n",
    "            plot_layer_name = plot_lookup[suffix]\n",
    "            print(f\"Processing: {yolo_name} matches Plot {plot_layer_name}\")\n",
    "            \n",
    "            # Load Data\n",
    "            gdf_yolo = gpd.read_file(fp)\n",
    "            gdf_plot = gpd.read_file(INPUT_PLOT_GDB, layer=plot_layer_name, engine='pyogrio')\n",
    "            \n",
    "            if gdf_plot.empty:\n",
    "                print(f\"  [!] Plot layer empty. Copying original.\")\n",
    "                gdf_yolo.to_file(os.path.join(OUTPUT_FOLDER, filename))\n",
    "                continue\n",
    "\n",
    "            # Align CRS\n",
    "            target_crs = gdf_plot.crs\n",
    "            if gdf_yolo.crs != target_crs:\n",
    "                gdf_yolo = gdf_yolo.to_crs(target_crs)\n",
    "            \n",
    "            # Clip VARI\n",
    "            if gdf_vari_master.crs != target_crs:\n",
    "                gdf_plot_vari_crs = gdf_plot.to_crs(gdf_vari_master.crs)\n",
    "                gdf_vari_clipped = gpd.clip(gdf_vari_master, gdf_plot_vari_crs)\n",
    "                if not gdf_vari_clipped.empty:\n",
    "                    gdf_vari_clipped = gdf_vari_clipped.to_crs(target_crs)\n",
    "            else:\n",
    "                gdf_vari_clipped = gpd.clip(gdf_vari_master, gdf_plot)\n",
    "\n",
    "            if gdf_vari_clipped.empty:\n",
    "                print(\"  No VARI data in this plot. Saving original.\")\n",
    "                gdf_yolo.to_file(os.path.join(OUTPUT_FOLDER, filename))\n",
    "                continue\n",
    "\n",
    "            # Erase\n",
    "            gdf_erased = gpd.overlay(gdf_vari_clipped, gdf_yolo, how='difference')\n",
    "\n",
    "            if gdf_erased.empty:\n",
    "                print(\"  No missing plants found.\")\n",
    "                gdf_final_combined = gdf_yolo\n",
    "            else:\n",
    "                # Subdivide\n",
    "                gdf_erased['area_sqm'] = gdf_erased.geometry.area\n",
    "                small_polys = gdf_erased[gdf_erased['area_sqm'] <= SUBDIVIDE_THRESHOLD].copy()\n",
    "                large_polys = gdf_erased[gdf_erased['area_sqm'] > SUBDIVIDE_THRESHOLD].copy()\n",
    "                \n",
    "                new_chunks = []\n",
    "                if not large_polys.empty:\n",
    "                    print(f\"  Subdividing {len(large_polys)} large polygons...\")\n",
    "                    for geom in large_polys.geometry:\n",
    "                        chunks = subdivide_polygon(geom, SUBDIVIDE_THRESHOLD)\n",
    "                        new_chunks.extend(chunks)\n",
    "                \n",
    "                if new_chunks:\n",
    "                    gdf_subdivided = gpd.GeoDataFrame(geometry=new_chunks, crs=target_crs)\n",
    "                    gdf_final_vari = pd.concat([small_polys, gdf_subdivided], ignore_index=True)\n",
    "                else:\n",
    "                    gdf_final_vari = small_polys\n",
    "\n",
    "                # Combine VARI + YOLO (YOLO attributes exist here, VARI has NaNs)\n",
    "                gdf_final_combined = pd.concat([gdf_yolo, gdf_final_vari], ignore_index=True)\n",
    "\n",
    "            # --- ITERATIVE ELIMINATION (With Attribute Preservation) ---\n",
    "            current_gdf = gdf_final_combined\n",
    "            for i in range(ELIMINATION_PASSES):\n",
    "                exploded_check = current_gdf.explode(index_parts=False)\n",
    "                num_small = len(exploded_check[exploded_check.geometry.area < MIN_AREA_THRESHOLD])\n",
    "                if num_small == 0:\n",
    "                    break\n",
    "                current_gdf = eliminate_pass(current_gdf, MIN_AREA_THRESHOLD, i+1)\n",
    "\n",
    "            # --- NEW: HANDLE CLASS VALUES ---\n",
    "            # Check if 'cls' column exists (standard in YOLO export)\n",
    "            # If not, try to find a column that looks like a class ID, or skip\n",
    "            target_col = 'cls'\n",
    "            \n",
    "            if target_col not in current_gdf.columns:\n",
    "                # Fallback: Check for 'class', 'Class', 'gridcode' or create 'cls'\n",
    "                possible_cols = [c for c in current_gdf.columns if c.lower() in ['class', 'grid_code', 'label']]\n",
    "                if possible_cols:\n",
    "                    target_col = possible_cols[0]\n",
    "                else:\n",
    "                    print(f\"    - Warning: 'cls' column not found. Creating it.\")\n",
    "                    current_gdf[target_col] = None # Create empty\n",
    "\n",
    "            # Fill NaN/None with 0\n",
    "            print(f\"    - Filling missing '{target_col}' values with 0...\")\n",
    "            current_gdf[target_col] = current_gdf[target_col].fillna(0)\n",
    "            \n",
    "            # Also ensure it's integer type if possible\n",
    "            try:\n",
    "                current_gdf[target_col] = current_gdf[target_col].astype(int)\n",
    "            except:\n",
    "                pass # Keep as is if conversion fails\n",
    "\n",
    "            # --- JOIN PLOT ATTRIBUTES ---\n",
    "            print(\"    - Joining Plot Attributes...\")\n",
    "            gdf_attributed = gpd.sjoin(current_gdf, gdf_plot, how='left', predicate='intersects')\n",
    "            \n",
    "            if 'index_right' in gdf_attributed.columns:\n",
    "                gdf_attributed = gdf_attributed.drop(columns=['index_right'])\n",
    "\n",
    "            # Save\n",
    "            out_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            gdf_attributed.to_file(out_path)\n",
    "            print(f\"  Success: Saved to {out_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"FAILED on {yolo_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n=== Processing Complete ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dda4a1-4010-4910-83f5-8f6424ae4d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190fa359-6e20-4ce3-8c21-9398724cf0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
