{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa3423d-e2ca-4861-9f76-0fe6b04ba30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Trait: CWT_A ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 261, but rank is 127\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          sum_sq      df          F        PR(>F)\n",
      "C_ENTRY             17934.519980   261.0   2.388770  2.989245e-15\n",
      "C_Location           2278.672297     1.0  79.214899  1.014813e-18\n",
      "C_REP                 492.140920     3.0   5.702867  6.881104e-04\n",
      "C_ENTRY:C_Location   9432.124199   261.0   1.256302  4.809371e-03\n",
      "Residual            75625.034127  2629.0        NaN           NaN\n",
      "--- Processing Trait: CorrectedYield ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bazrafka\\AppData\\Local\\Temp\\ipykernel_62668\\1598798821.py:43: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  sample_sizes = df.groupby('ENTRY').size()\n",
      "C:\\Users\\bazrafka\\AppData\\Local\\Temp\\ipykernel_62668\\1598798821.py:54: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  summary_df = df.groupby('ENTRY')[trait].mean().reset_index()\n",
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 261, but rank is 127\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          sum_sq      df          F        PR(>F)\n",
      "C_ENTRY             16806.664635   261.0   2.250837  2.910939e-13\n",
      "C_Location           1577.658632     1.0  55.146262  1.504200e-13\n",
      "C_REP                 659.949963     3.0   7.689406  4.093292e-05\n",
      "C_ENTRY:C_Location   7664.652220   261.0   1.026491  3.779584e-01\n",
      "Residual            75097.636609  2625.0        NaN           NaN\n",
      "\n",
      "Analysis complete! Results saved to 'Agricultural_Analysis_Summary.xlsx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bazrafka\\AppData\\Local\\Temp\\ipykernel_62668\\1598798821.py:43: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  sample_sizes = df.groupby('ENTRY').size()\n",
      "C:\\Users\\bazrafka\\AppData\\Local\\Temp\\ipykernel_62668\\1598798821.py:54: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  summary_df = df.groupby('ENTRY')[trait].mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy import stats\n",
    "\n",
    "# 1. Load the dataset\n",
    "# Replace with your actual file path\n",
    "file_path = \"C:/Users/bazrafka/Desktop/counting/DiscussionPaperData/forLovepreet2.xlsx\" \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 2. Setup Factors\n",
    "# Ensure ENTRY, Location, and REP are treated as categorical factors\n",
    "df['ENTRY'] = df['ENTRY'].astype('category')\n",
    "df['Location'] = df['Location'].astype('category')\n",
    "df['REP'] = df['REP'].astype('category')\n",
    "\n",
    "# List of traits you want to analyze\n",
    "traits = ['CWT_A', 'CorrectedYield']\n",
    "\n",
    "# Prepare an Excel writer to save all results in one file\n",
    "with pd.ExcelWriter(\"Agricultural_Analysis_Summary.xlsx\") as writer:\n",
    "    \n",
    "    for trait in traits:\n",
    "        print(f\"--- Processing Trait: {trait} ---\")\n",
    "        \n",
    "        # 3. Perform ANOVA\n",
    "        # Formula: Trait ~ ENTRY * Location + REP\n",
    "        formula = f\"{trait} ~ C_ENTRY * C_Location + C_REP\"\n",
    "        # Renaming temporarily for statsmodels compatibility (factors denoted by C())\n",
    "        model_data = df.rename(columns={'ENTRY': 'C_ENTRY', 'Location': 'C_Location', 'REP': 'C_REP'})\n",
    "        \n",
    "        model = ols(formula, data=model_data).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "        print(anova_table)\n",
    "\n",
    "        # 4. Calculate LSD manually (matching your R logic)\n",
    "        # Extract Mean Square Error (Residuals) and Degrees of Freedom\n",
    "        mse = anova_table.loc['Residual', 'sum_sq'] / anova_table.loc['Residual', 'df']\n",
    "        df_error = anova_table.loc['Residual', 'df']\n",
    "        \n",
    "        # Harmonic mean of sample sizes per ENTRY\n",
    "        sample_sizes = df.groupby('ENTRY').size()\n",
    "        harmonic_mean_n = len(sample_sizes) / np.sum(1.0 / sample_sizes)\n",
    "        \n",
    "        # Critical t-value at 95% confidence (alpha = 0.05)\n",
    "        alpha = 0.05\n",
    "        t_crit = stats.t.ppf(1 - alpha/2, df_error)\n",
    "        \n",
    "        # Compute LSD Value\n",
    "        lsd_value = t_crit * np.sqrt(2 * mse / harmonic_mean_n)\n",
    "        \n",
    "        # 5. Create Summary Dataframe (Means by ENTRY)\n",
    "        summary_df = df.groupby('ENTRY')[trait].mean().reset_index()\n",
    "        summary_df.columns = ['group', 'means']\n",
    "        summary_df['LSD_Threshold'] = lsd_value\n",
    "        \n",
    "        # Note: True grouping letters (a, b, c) usually require the 'bioinfokit' \n",
    "        # or manual implementation. For now, we provide the means and the LSD.\n",
    "        \n",
    "        # 6. Save to separate sheet in Excel\n",
    "        summary_df.to_excel(writer, sheet_name=trait, index=False)\n",
    "\n",
    "print(\"\\nAnalysis complete! Results saved to 'Agricultural_Analysis_Summary.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adaf011-5001-4c94-8478-094393667ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac53e53-47f1-438b-a661-a57a99455a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not process 2217 for CWT_A: must have at least one row in constraint matrix\n",
      "Could not process 2217 for CorrectedYield: must have at least one row in constraint matrix\n",
      "Could not process 2218 for CWT_A: must have at least one row in constraint matrix\n",
      "Could not process 2218 for CorrectedYield: must have at least one row in constraint matrix\n",
      "Could not process 2219 for CWT_A: must have at least one row in constraint matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 127, but rank is 15\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 127, but rank is 15\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 127, but rank is 23\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not process 2219 for CorrectedYield: must have at least one row in constraint matrix\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 127, but rank is 23\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# 1. Load data\n",
    "#file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\forLovepreet2.xlsx\"\n",
    "file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\mrc22.xlsx\"\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure factors are categorical\n",
    "for col in ['ENTRY', 'Location', 'REP', 'ExperimentName']:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "traits = ['CWT_A', 'CorrectedYield']\n",
    "results_list = []\n",
    "\n",
    "# 2. Group by Experiment Name\n",
    "for exp_name, group_data in df.groupby('ExperimentName'):\n",
    "    # Skip if group is too small to run ANOVA\n",
    "    if len(group_data) < 10:\n",
    "        continue\n",
    "        \n",
    "    for trait in traits:\n",
    "        try:\n",
    "            # 3. Fit ANOVA for this specific Experiment\n",
    "            # Formula: Trait ~ ENTRY + Location + REP (Simplified if only 1 location)\n",
    "            formula = f\"Q('{trait}') ~ C(ENTRY) + C(Location) + C(REP)\"\n",
    "            model = ols(formula, data=group_data).fit()\n",
    "            anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "            \n",
    "            # 4. Extract Statistics\n",
    "            mse = anova_table.loc['Residual', 'sum_sq'] / anova_table.loc['Residual', 'df']\n",
    "            grand_mean = group_data[trait].mean()\n",
    "            \n",
    "            # 5. Calculate %CV\n",
    "            cv_value = (np.sqrt(mse) / grand_mean) * 100\n",
    "            \n",
    "            # 6. Store results\n",
    "            results_list.append({\n",
    "                'Experiment': exp_name,\n",
    "                'Trait': trait,\n",
    "                'Grand_Mean': round(grand_mean, 2),\n",
    "                'MSE': round(mse, 4),\n",
    "                'Percent_CV': round(cv_value, 2)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {exp_name} for {trait}: {e}\")\n",
    "\n",
    "# 7. Create Summary Table and Save\n",
    "cv_summary = pd.DataFrame(results_list)\n",
    "cv_summary.to_excel(\"Experiment_CV_Results.xlsx\", index=False)\n",
    "print(cv_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25b972-db9f-44dd-9086-44bf28f896aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c3ff13-dfde-42c0-94df-1a4020ce8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first remove outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "315dc946-4388-4cea-ba91-21eb5230ce9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not process 2217 for CWT_A: must have at least one row in constraint matrix\n",
      "Could not process 2217 for CorrectedYield: must have at least one row in constraint matrix\n",
      "Could not process 2218 for CWT_A: must have at least one row in constraint matrix\n",
      "Could not process 2218 for CorrectedYield: must have at least one row in constraint matrix\n",
      "Could not process 2219 for CWT_A: must have at least one row in constraint matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 127, but rank is 15\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 127, but rank is 15\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 127, but rank is 23\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not process 2219 for CorrectedYield: must have at least one row in constraint matrix\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 127, but rank is 23\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# --- OUTLIER REMOVAL FUNCTION ---\n",
    "def remove_outliers(data, column):\n",
    "    \"\"\"Removes outliers based on the 1.5 * IQR rule.\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    # Keep only data within bounds\n",
    "    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "\n",
    "# 1. Load data\n",
    "file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\mrc22.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure factors are categorical\n",
    "for col in ['ENTRY', 'Location', 'REP', 'ExperimentName']:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "traits = ['CWT_A', 'CorrectedYield']\n",
    "results_list = []\n",
    "\n",
    "# 2. Group by Experiment Name\n",
    "for exp_name, group_data in df.groupby('ExperimentName'):\n",
    "    if len(group_data) < 10:\n",
    "        continue\n",
    "        \n",
    "    for trait in traits:\n",
    "        try:\n",
    "            # --- STEP: REMOVE OUTLIERS FOR THIS SPECIFIC TRAIT/EXPERIMENT ---\n",
    "            # We clean the data specifically for the trait we are about to analyze\n",
    "            clean_data = remove_outliers(group_data.copy(), trait)\n",
    "            \n",
    "            # 3. Fit ANOVA for this specific Experiment using CLEANED data\n",
    "            formula = f\"Q('{trait}') ~ C(ENTRY) + C(Location) + C(REP)\"\n",
    "            model = ols(formula, data=clean_data).fit()\n",
    "            anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "            \n",
    "            # 4. Extract Statistics\n",
    "            mse = anova_table.loc['Residual', 'sum_sq'] / anova_table.loc['Residual', 'df']\n",
    "            grand_mean = clean_data[trait].mean()\n",
    "            \n",
    "            # 5. Calculate %CV\n",
    "            cv_value = (np.sqrt(mse) / grand_mean) * 100\n",
    "            \n",
    "            # 6. Store results\n",
    "            results_list.append({\n",
    "                'Experiment': exp_name,\n",
    "                'Trait': trait,\n",
    "                'Original_Count': len(group_data),\n",
    "                'Cleaned_Count': len(clean_data),\n",
    "                'Grand_Mean': round(grand_mean, 2),\n",
    "                'MSE': round(mse, 4),\n",
    "                'Percent_CV': round(cv_value, 2)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {exp_name} for {trait}: {e}\")\n",
    "\n",
    "# 7. Create Summary Table and Save\n",
    "cv_summary = pd.DataFrame(results_list)\n",
    "cv_summary.to_excel(\"Experiment_CV_Results_Cleaned.xlsx\", index=False)\n",
    "print(cv_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388f159-0560-4f05-aab8-18424fabd98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "785fb0bd-3b08-42ad-ad6d-ebac6a0084fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Results saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def remove_outliers(data, column):\n",
    "    if data[column].isnull().all():\n",
    "        return data\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return data[(data[column] >= (Q1 - 1.5 * IQR)) & (data[column] <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "#file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\mrc22.xlsx\"\n",
    "file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\MRC2022_\\EXCEL\\Joined\\All_Trials_Combined.xlsx\"\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "for col in ['ENTRY', 'Location', 'REP', 'ExperimentName']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "traits = ['CWT_A', 'CorrectedYield']\n",
    "results_list = []\n",
    "\n",
    "for exp_name, group_data in df.groupby('ExperimentName', observed=False):\n",
    "    if len(group_data) < 5: # Lowered threshold to see if we get anything\n",
    "        continue\n",
    "        \n",
    "    for trait in traits:\n",
    "        if trait not in group_data.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            clean_data = remove_outliers(group_data.copy(), trait)\n",
    "            \n",
    "            # --- DYNAMIC FORMULA BUILDING ---\n",
    "            # Start with ENTRY, add others ONLY if they have more than 1 level\n",
    "            formula_parts = [\"C(ENTRY)\"]\n",
    "            \n",
    "            if clean_data['Location'].nunique() > 1:\n",
    "                formula_parts.append(\"C(Location)\")\n",
    "            if clean_data['REP'].nunique() > 1:\n",
    "                formula_parts.append(\"C(REP)\")\n",
    "            \n",
    "            formula = f\"Q('{trait}') ~ \" + \" + \".join(formula_parts)\n",
    "            \n",
    "            # Fit model\n",
    "            model = ols(formula, data=clean_data).fit()\n",
    "            anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "            \n",
    "            mse = anova_table.loc['Residual', 'sum_sq'] / anova_table.loc['Residual', 'df']\n",
    "            grand_mean = clean_data[trait].mean()\n",
    "            cv_value = (np.sqrt(mse) / grand_mean) * 100\n",
    "            \n",
    "            results_list.append({\n",
    "                'Experiment': exp_name,\n",
    "                'Trait': trait,\n",
    "                'Percent_CV': round(cv_value, 2),\n",
    "                'Grand_Mean': round(grand_mean, 2),\n",
    "                'Formula_Used': formula\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {exp_name} for {trait}: {e}\")\n",
    "\n",
    "if results_list:\n",
    "    cv_summary = pd.DataFrame(results_list)\n",
    "    cv_summary.to_excel(\"Experiment_CV_Results_Simplified.xlsx\", index=False)\n",
    "    print(\"\\nSuccess! Results saved.\")\n",
    "else:\n",
    "    print(\"\\nStill no results. Check if CWT_A and CorrectedYield contain numbers or if they are all empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68572d-3f98-4be7-9e80-40644c62bd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce877ad7-a32f-4ba9-9e58-de964d291ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe107c24-1dcf-44eb-95c2-3f14d973c0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:1634: UserWarning: Random effects covariance is singular\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Analysis complete.\n",
      "   Experiment           Trait  Percent_CV  Grand_Mean      MSE  \\\n",
      "0        2217           CWT_A       16.34       19.92  10.5948   \n",
      "1        2217  CorrectedYield       16.99       15.97   7.3615   \n",
      "2        2218           CWT_A       16.12       21.43  11.9370   \n",
      "3        2218  CorrectedYield       16.38       17.15   7.8913   \n",
      "4        2219           CWT_A       13.83       19.66   7.3923   \n",
      "5        2219  CorrectedYield       13.72       15.08   4.2842   \n",
      "\n",
      "                  Model_Type  \n",
      "0  Mixed Model (Random IBLK)  \n",
      "1  Mixed Model (Random IBLK)  \n",
      "2               OLS Fallback  \n",
      "3               OLS Fallback  \n",
      "4               OLS Fallback  \n",
      "5               OLS Fallback  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings\n",
    "\n",
    "# Suppress technical warnings for cleaner output\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', message='covariance of constraints does not have full rank')\n",
    "warnings.filterwarnings('ignore', category=sm.tools.sm_exceptions.ConvergenceWarning)\n",
    "\n",
    "# --- OUTLIER REMOVAL FUNCTION ---\n",
    "def remove_outliers(data, column):\n",
    "    \"\"\"Removes outliers based on the 1.5 * IQR rule.\"\"\"\n",
    "    if data[column].isnull().all():\n",
    "        return data\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return data[(data[column] >= lower) & (data[column] <= upper)]\n",
    "\n",
    "# 1. Load data\n",
    "file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\MRC2022_\\EXCEL\\Joined\\All_Trials_Combined.xlsx\"\n",
    "\n",
    "#file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\2021_\\excel\\Joined\\All_Trials_Combined.xlsx\"\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure factors are categorical\n",
    "# Using 'IBLK' for incomplete blocks to reduce CV\n",
    "for col in ['ENTRY', 'Location', 'REP', 'IBLK', 'ExperimentName']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "traits = ['CWT_A', 'CorrectedYield']\n",
    "results_list = []\n",
    "CLEAN_OUTLIERS = False  # Set to True if you want to remove outliers first\n",
    "\n",
    "# 2. Group by Experiment Name\n",
    "for exp_name, group_data in df.groupby('ExperimentName', observed=False):\n",
    "    if len(group_data) < 10: \n",
    "        continue\n",
    "        \n",
    "    for trait in traits:\n",
    "        if trait not in group_data.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Prepare data\n",
    "            analysis_data = remove_outliers(group_data.copy(), trait) if CLEAN_OUTLIERS else group_data.copy()\n",
    "            \n",
    "            # Check for variation\n",
    "            if analysis_data[trait].std() == 0 or analysis_data[trait].isnull().all():\n",
    "                continue\n",
    "\n",
    "            # --- DYNAMIC FORMULA BUILDING ---\n",
    "            fixed_parts = [\"C(ENTRY)\"]\n",
    "            if analysis_data['Location'].nunique() > 1:\n",
    "                fixed_parts.append(\"C(Location)\")\n",
    "            if analysis_data['REP'].nunique() > 1:\n",
    "                fixed_parts.append(\"C(REP)\")\n",
    "            \n",
    "            fixed_formula = f\"Q('{trait}') ~ \" + \" + \".join(fixed_parts)\n",
    "            \n",
    "            # 3. Fit Mixed Linear Model (REML)\n",
    "            # We use 'lbfgs' and high maxiter to help with ConvergenceWarnings\n",
    "            model = smf.mixedlm(fixed_formula, analysis_data, groups=analysis_data[\"REP\"], re_formula=\"~0 + C(IBLK)\")\n",
    "            result = model.fit(method=[\"lbfgs\"], maxiter=20000)\n",
    "            \n",
    "            # 4. Extract Statistics for %CV\n",
    "            # mse = result.scale (Residual Variance)\n",
    "            mse = result.scale \n",
    "            grand_mean = analysis_data[trait].mean()\n",
    "            cv_value = (np.sqrt(mse) / grand_mean) * 100\n",
    "            \n",
    "            results_list.append({\n",
    "                'Experiment': exp_name,\n",
    "                'Trait': trait,\n",
    "                'Percent_CV': round(cv_value, 2),\n",
    "                'Grand_Mean': round(grand_mean, 2),\n",
    "                'MSE': round(mse, 4),\n",
    "                'Model_Type': \"Mixed Model (Random IBLK)\"\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            # FALLBACK: If Mixed Model fails to converge/run, use standard OLS\n",
    "            try:\n",
    "                formula_ols = f\"Q('{trait}') ~ C(ENTRY) + C(REP)\"\n",
    "                model_ols = smf.ols(formula_ols, data=analysis_data).fit()\n",
    "                mse_ols = model_ols.mse_resid\n",
    "                cv_ols = (np.sqrt(mse_ols) / analysis_data[trait].mean()) * 100\n",
    "                \n",
    "                results_list.append({\n",
    "                    'Experiment': exp_name,\n",
    "                    'Trait': trait,\n",
    "                    'Percent_CV': round(cv_ols, 2),\n",
    "                    'Grand_Mean': round(analysis_data[trait].mean(), 2),\n",
    "                    'MSE': round(mse_ols, 4),\n",
    "                    'Model_Type': \"OLS Fallback\"\n",
    "                })\n",
    "            except:\n",
    "                print(f\"Skipping {exp_name} for {trait}: {e}\")\n",
    "\n",
    "# 5. Save Summary\n",
    "if results_list:\n",
    "    cv_summary = pd.DataFrame(results_list)\n",
    "    cv_summary.to_excel(\"Experiment_CV_MaxedModel_Final.xlsx\", index=False)\n",
    "    print(\"\\nSuccess! Analysis complete.\")\n",
    "    print(cv_summary.head(10))\n",
    "else:\n",
    "    print(\"\\nNo results generated. Check column names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de758bf-e370-48e7-bee4-6a8d4767de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To determine which part of the data affects the overall Coefficient of Variation (CV) more,\n",
    "#we need to split your data into \"Low,\" \"Medium,\" and \"High\" yield tiers (quantiles) and run \n",
    "#the Mixed Model analysis on each subset separately.\n",
    "\n",
    "#The following code calculates the local CV for these three tiers. \n",
    "#If the \"Low\" tier has a significantly higher CV than the \"High\" tier, it suggests that measurement errors or\n",
    "#plot variability at lower yield levels are the primary drivers of your overall noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec114e2-a5ac-4379-80a6-3a32301b7a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Average CV by Yield Tier ---\n",
      "Tier             High    Low  Medium\n",
      "Trait                               \n",
      "CWT_A           10.04  30.89    5.94\n",
      "CorrectedYield  15.83  21.09    6.43\n",
      "\n",
      "Analysis complete. Check 'CV_Yield_Tier_Analysis.xlsx' for details.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings\n",
    "\n",
    "# Suppress technical warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', message='covariance of constraints does not have full rank')\n",
    "warnings.filterwarnings('ignore', category=sm.tools.sm_exceptions.ConvergenceWarning)\n",
    "\n",
    "# --- OUTLIER REMOVAL FUNCTION ---\n",
    "def remove_outliers(data, column):\n",
    "    if data[column].isnull().all():\n",
    "        return data\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return data[(data[column] >= lower) & (data[column] <= upper)]\n",
    "\n",
    "# 1. Load data\n",
    "#file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\MRC2022_\\EXCEL\\Joined\\All_Trials_Combined.xlsx\"\n",
    "#file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\2024_\\excell\\Joined\\All_Trials_Combined.xlsx\"\n",
    "#file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\SEVREC2022_\\excel\\Joined\\All_Trials_Combined.xlsx\"\n",
    "#file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\sevrec25_\\excel\\Joined\\All_Trials_Combined.xlsx\"\n",
    "#file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\2021_\\excel\\Joined\\All_Trials_Combined.xlsx\"\n",
    "#file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\AllYearsLocationsExceptCanadaNEW.xlsx\"\n",
    "#file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\Canada25\\excel\\Joined\\All_Trials_Combined.xlsx\"\n",
    "file_path = r\"C:\\Users\\bazrafka\\Desktop\\counting\\DiscussionPaperData\\Outputs\\Canada24\\excel\\Joined\\All_Trials_Combined.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "for col in ['ENTRY', 'Location', 'REP', 'IBLK', 'Experiment Name']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "traits = ['CWT_A', 'CorrectedYield']\n",
    "results_list = []\n",
    "CLEAN_OUTLIERS = False \n",
    "\n",
    "# 2. Group by Experiment Name\n",
    "for exp_name, group_data in df.groupby('Experiment Name', observed=False):\n",
    "    if len(group_data) < 15: # Increased threshold to ensure enough data for tiers\n",
    "        continue\n",
    "        \n",
    "    for trait in traits:\n",
    "        if trait not in group_data.columns:\n",
    "            continue\n",
    "            \n",
    "        analysis_data = remove_outliers(group_data.copy(), trait) if CLEAN_OUTLIERS else group_data.copy()\n",
    "        \n",
    "        # --- NEW STEP: SPLIT INTO TIERS ---\n",
    "        # We define Low (Bottom 33%), Medium (33-66%), and High (Top 33%) yield tiers\n",
    "        try:\n",
    "            analysis_data['Yield_Tier'] = pd.qcut(analysis_data[trait], 3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "        except ValueError: # If data is too uniform to split\n",
    "            continue\n",
    "\n",
    "        for tier in [\"Low\", \"Medium\", \"High\"]:\n",
    "            tier_data = analysis_data[analysis_data['Yield_Tier'] == tier].copy()\n",
    "            \n",
    "            if len(tier_data) < 5: continue # Skip if tier is too small\n",
    "            \n",
    "            try:\n",
    "                # Dynamic Formula\n",
    "                fixed_parts = [\"C(ENTRY)\"]\n",
    "                if tier_data['REP'].nunique() > 1:\n",
    "                    fixed_parts.append(\"C(REP)\")\n",
    "                \n",
    "                fixed_formula = f\"Q('{trait}') ~ \" + \" + \".join(fixed_parts)\n",
    "                \n",
    "                # Fit Mixed Model\n",
    "                model = smf.mixedlm(fixed_formula, tier_data, groups=tier_data[\"REP\"], re_formula=\"~0 + C(IBLK)\")\n",
    "                result = model.fit(method=[\"lbfgs\"], maxiter=20000)\n",
    "                \n",
    "                mse = result.scale \n",
    "                mean_val = tier_data[trait].mean()\n",
    "                cv_value = (np.sqrt(mse) / mean_val) * 100\n",
    "                \n",
    "                results_list.append({\n",
    "                    'Experiment': exp_name,\n",
    "                    'Trait': trait,\n",
    "                    'Tier': tier,\n",
    "                    'Percent_CV': round(cv_value, 2),\n",
    "                    'Mean_Yield': round(mean_val, 2),\n",
    "                    'MSE': round(mse, 4)\n",
    "                })\n",
    "                \n",
    "            except Exception:\n",
    "                # OLS Fallback for Tiers\n",
    "                try:\n",
    "                    formula_ols = f\"Q('{trait}') ~ C(ENTRY)\"\n",
    "                    model_ols = smf.ols(formula_ols, data=tier_data).fit()\n",
    "                    mse_ols = model_ols.mse_resid\n",
    "                    cv_ols = (np.sqrt(mse_ols) / tier_data[trait].mean()) * 100\n",
    "                    \n",
    "                    results_list.append({\n",
    "                        'Experiment': exp_name,\n",
    "                        'Trait': trait,\n",
    "                        'Tier': tier,\n",
    "                        'Percent_CV': round(cv_ols, 2),\n",
    "                        'Mean_Yield': round(tier_data[trait].mean(), 2),\n",
    "                        'MSE': round(mse_ols, 4)\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "# 3. Save and Summarize\n",
    "if results_list:\n",
    "    tier_summary = pd.DataFrame(results_list)\n",
    "    tier_summary.to_excel(\"CV_Yield_Tier_Analysis.xlsx\", index=False)\n",
    "    \n",
    "    # Calculate group averages to see the trend\n",
    "    trend = tier_summary.groupby(['Trait', 'Tier'])['Percent_CV'].mean().unstack()\n",
    "    print(\"\\n--- Average CV by Yield Tier ---\")\n",
    "    print(trend)\n",
    "    print(\"\\nAnalysis complete. Check 'CV_Yield_Tier_Analysis.xlsx' for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b4f25-d3ed-4704-bf31-edab66cada12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e72691-5cec-4939-8933-1b2edea009b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
